{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Answer 2\n",
        "##part(A)\n",
        "To find the minimum value of $q_2(x)$, we have already derived the expression for $q_2(x)$:\n",
        "\n",
        "$q_2(x) = 2q_1(x_1) + (2q_2 - 1)x_1 + \\left(\\frac{3}{2}q_2^2 - 2q_2\\right)$\n",
        "\n",
        "We can rewrite this expression in matrix form as:\n",
        "\n",
        "$q_2(x) = \\frac{1}{2}x^\\top Ax - b_2^\\top x$\n",
        "\n",
        "Where:\n",
        "\n",
        "$A = \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix}$\n",
        "\n",
        "$b_2 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$\n",
        "\n",
        "The minimum value of $q_2(x)$ occurs at the minimizer, which we can find by setting the gradient of $q_2(x)$ with respect to $x$ equal to zero:\n",
        "\n",
        "$\\nabla q_2(x) = 0$\n",
        "\n",
        "$\\Rightarrow Ax - b_2 = 0$\n",
        "\n",
        "Solving this equation gives us the minimizer $x^*$:\n",
        "\n",
        "$x^* = A^{-1}b_2$\n",
        "\n",
        "We already know that $A$ is invertible, so we can compute $A^{-1}$ and find $x^*$.\n",
        "\n",
        "Once we have $x^*$, we can substitute it into $q_2(x)$ to find the minimum value of $q_2(x)$.\n",
        "\n",
        "Let's proceed with these calculations. First, let's find $A^{-1}$:\n",
        "\n",
        "$A^{-1} = \\frac{1}{\\text{det}(A)} \\begin{pmatrix} 3 & -1 \\\\ -1 & 4 \\end{pmatrix}$\n",
        "\n",
        "where $\\text{det}(A) = 11$.\n",
        "\n",
        "So,\n",
        "\n",
        "$A^{-1} = \\frac{1}{11} \\begin{pmatrix} 3 & -1 \\\\ -1 & 4 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{11} & -\\frac{1}{11} \\\\ -\\frac{1}{11} & \\frac{4}{11} \\end{pmatrix}$\n",
        "\n",
        "Now,\n",
        "\n",
        "$x^* = A^{-1}b_2 = \\begin{pmatrix} \\frac{3}{11} & -\\frac{1}{11} \\\\ -\\frac{1}{11} & \\frac{4}{11} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{11} \\\\ \\frac{6}{11} \\end{pmatrix}$\n",
        "\n",
        "Now, substitute $x^*$ into $q_2(x)$ to find the minimum value:\n",
        "\n",
        "$q_2(x^*) = 2 \\begin{pmatrix} \\frac{1}{11} \\end{pmatrix}^\\top A \\begin{pmatrix} \\frac{1}{11} \\end{pmatrix} - \\begin{pmatrix} 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{11} \\\\ \\frac{6}{11} \\end{pmatrix}$\n",
        "\n",
        "$= 2 \\left( \\frac{1}{11} \\right)^2 \\begin{pmatrix} 4 & 1 \\\\ 1 & 3 \\end{pmatrix} - \\left( \\frac{1}{11} + \\frac{12}{11} \\right)$\n",
        "\n",
        "$= \\frac{1}{121} \\begin{pmatrix} 8 & 2 \\\\ 2 & 6 \\end{pmatrix} - \\frac{13}{11}$\n",
        "\n",
        "$= \\frac{1}{121} \\begin{pmatrix} 8 & 2 \\\\ 2 & 6 \\end{pmatrix} - \\frac{13}{11}$\n",
        "\n",
        "$= \\frac{8}{121} + \\frac{6}{121} - \\frac{13}{11}$\n",
        "\n",
        "$= \\frac{57}{121}$\n",
        "\n",
        "So, the minimum value of $q_2(x)$ is $\\frac{57}{121}$.\n",
        "\n",
        "Given:\n",
        "\n",
        "$b_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n",
        "\n",
        "The function $q_1(x)$ is:\n",
        "\n",
        "$q_1(x) = \\frac{1}{2} x^\\top Wx - b_1^\\top x$\n",
        "\n",
        "Substituting $b_1$ into $q_1(x)$:\n",
        "\n",
        "$q_1(x) = \\frac{1}{2} x^\\top Wx - \\begin{pmatrix} 1 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$\n",
        "\n",
        "$= \\frac{1}{2} x^\\top Wx - x_1$\n",
        "\n",
        "Now, let's find the gradient of $q_1(x)$ with respect to $x$:\n",
        "\n",
        "$\\nabla q_1(x) = \\frac{\\partial}{\\partial x} \\left( \\frac{1}{2} x^\\top Wx - x_1 \\right)$\n",
        "\n",
        "$= \\frac{1}{2} (W + W^\\top) x - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n",
        "\n",
        "Setting the gradient to zero:\n",
        "\n",
        "$\\frac{1}{2} (W + W^\\top) x - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 0$\n",
        "\n",
        "$(W + W^\\top) x = 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n",
        "\n",
        "Now, we can substitute the given value of $W$ into the equation and solve for $x^*$:\n",
        "\n",
        "$\\begin{pmatrix} 2t & 2\\sqrt{t} \\\\ 2\\sqrt{t} & 2+2t \\end{pmatrix} x = 2 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n",
        "\n",
        "$\\begin{pmatrix} t & \\sqrt{t} \\\\ \\sqrt{t} & 1+t \\end{pmatrix} x = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$\n",
        "\n",
        "This system can be written as two equations:\n",
        "\n",
        "$t x_1 + \\sqrt{t} x_2 = 1$\n",
        "\n",
        "$\\sqrt{t} x_1 + (1+t) x_2 = 0$\n",
        "\n",
        "Solving these equations will give us the minimizer $x^*$. Once we have $x^*$, we can substitute it into $q_1(x)$ to find the minimum value of $q_1(x)$.\n",
        "\n",
        "First, let's multiply the first equation by $t$ and subtract the second equation from it to eliminate $x_1$:\n",
        "\n",
        "\\begin{align*}\n",
        "(t\\cdot 1 - 0) - \\left(t \\cdot \\frac{2}{3} - (1+t)\\right)x_2 &= t - t^2x_2 - tx_2 - (1+t)x_2 \\\\\n",
        "&= tx_2 - (1+t)x_2 \\\\\n",
        "&= t - tx_2 = t(1 - x_2) = -tx_2\n",
        "\\end{align*}\n",
        "\n",
        "So, substituting $x_2 = -\\frac{t}{t}$ into the first equation, we find:\n",
        "\n",
        "$\n",
        "\\frac{2}{t}x_1 = 2\n",
        "$\n",
        "\n",
        "Therefore, the minimizer $x^*$ is:\n",
        "\n",
        "$\n",
        "x^* = \\begin{pmatrix} \\frac{2}{t} \\\\ -\\frac{t}{t} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{t} \\\\ -1 \\end{pmatrix}\n",
        "$\n",
        "\n",
        "\n",
        "The minimum value of $q_1(x)$ for $t \\neq 0$ is $\\frac{1}{t^2} - \\frac{t}{2} - \\sqrt{t} + 1$.\n",
        "\n",
        "*** Is the minimiser Unique***\n",
        "\n",
        "Here we Can oobserve that minimiser is Unique for both of the Functions\n",
        "\n",
        "*** Are They local or global Minima***\n",
        "\n",
        "For $q_1(x)$ and $q_2(x)$:\n",
        "\n",
        "$q_1(x)$: This is a quadratic function of $x$. Since the coefficient matrix $W$ is symmetric positive definite, $q_1(x)$ is convex. In convex optimization, any local minimum is also a global minimum. Therefore, the minimum obtained for $q_1(x)$ is a global minimum.\n",
        "\n",
        "$q_2(x)$: Similar to $q_1(x)$, $q_2(x)$ is also a quadratic function of $x$. Since $A$ is symmetric positive definite, $q_2(x)$ is also convex. Hence, any local minimum for $q_2(x)$ is a global minimum.\n",
        "\n",
        "In summary, the minima obtained for both $q_1(x)$ and $q_2(x)$ are global minima.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "***Are q1 and q2 are convex ***\n",
        "\n",
        "To determine if the functions $q_1(x)$ and $q_2(x)$ are convex, we need to analyze their Hessian matrices. A function is convex if its Hessian matrix is positive semidefinite for all $x$ in its domain.\n",
        "\n",
        "For $q_1(x)$:\n",
        "\n",
        "$ q_1(x) = \\frac{1}{2} x^\\top Wx - b_1^\\top x $\n",
        "\n",
        "The Hessian matrix $H$ of $q_1(x)$ is simply $W$ since there is no cross-term in the quadratic form. Given the structure of $W$, it's not immediately clear if $W$ is positive semidefinite. However, the given matrix $W$ is generally not symmetric, which is a requirement for a positive semidefinite matrix.\n",
        "\n",
        "For $q_2(x)$:\n",
        "\n",
        "$ q_2(x) = \\frac{1}{2} x^\\top Ax - b_2^\\top x $\n",
        "\n",
        "Here, the Hessian matrix $H$ is $A$. Since $A$ is symmetric and positive definite, it is also positive semidefinite.\n",
        "\n",
        "In conclusion:\n",
        "- $q_1(x)$ may not be convex due to the nonsymmetric nature of $W$.\n",
        "- $q_2(x)$ is convex since $A$ is symmetric and positive definite.\n",
        "\n",
        "Therefore, only $q_2(x)$ is convex.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CszxcLT3-6Qn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Py1685v2-4v2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4tsBjZin-5Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part(b)"
      ],
      "metadata": {
        "id": "_v1UqOoZKgWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function q1(x)\n",
        "def q1(x):\n",
        "    W = np.array([[t, np.sqrt(t)], [np.sqrt(t), 1 + t]])\n",
        "    b1 = np.array([1, 0])\n",
        "    return 0.5 * np.dot(np.dot(x.T, W), x) - np.dot(b1.T, x)\n",
        "\n",
        "# Define the gradient of q1(x)\n",
        "def grad_q1(x):\n",
        "    W = np.array([[t, np.sqrt(t)], [np.sqrt(t), 1 + t]])\n",
        "    return np.dot(W + W.T, x) - np.array([1, 0])\n",
        "\n",
        "# Define the step size and momentum parameters\n",
        "def alpha(t):\n",
        "    return 2 / (3 + np.sqrt(9 - 4 * t**2))\n",
        "\n",
        "def beta(t):\n",
        "    mu0 = (3 + np.sqrt(9 - 4 * t**2)) / (3 - np.sqrt(9 - 4 * t**2))\n",
        "    return np.sqrt(mu0 - 1) / np.sqrt(mu0 + 1)\n",
        "\n",
        "# Initialize parameters\n",
        "x0 = np.array([3, 5])\n",
        "t = 0.001\n",
        "tau = 1e-5\n",
        "\n",
        "# Nesterov accelerated gradient descent\n",
        "x = x0\n",
        "x_prev = x0\n",
        "k = 0\n",
        "while np.linalg.norm(grad_q1(x)) > tau:\n",
        "    grad_pert = grad_q1(x + beta(t) * (x - x_prev))\n",
        "    x_next = x - alpha(t) * grad_pert + beta(t) * (x - x_prev)\n",
        "    x_prev = x\n",
        "    x = x_next\n",
        "    k += 1\n",
        "\n",
        "# Output the result\n",
        "print(\"Optimal solution x*:\", x)\n",
        "print(\"Number of iterations:\", k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxjcX6FDD7XZ",
        "outputId": "be480f4f-26f4-449d-a119-40ad196b2fad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution x*: [500499.95734358 -15811.38695327]\n",
            "Number of iterations: 175243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function q1(x)\n",
        "def q1(x):\n",
        "    W = np.array([[t, np.sqrt(t)], [np.sqrt(t), 1 + t]])\n",
        "    b1 = np.array([1, 0])\n",
        "    return 0.5 * np.dot(np.dot(x.T, W), x) - np.dot(b1.T, x)\n",
        "\n",
        "# Define the gradient of q1(x)\n",
        "def grad_q1(x):\n",
        "    W = np.array([[t, np.sqrt(t)], [np.sqrt(t), 1 + t]])\n",
        "    return np.dot(W + W.T, x) - np.array([1, 0])\n",
        "\n",
        "# Define the step size\n",
        "def alpha(t):\n",
        "    return 2 / (3 + np.sqrt(9 - 4 * t**2))\n",
        "\n",
        "# Initialize parameters\n",
        "x0 = np.array([3, 5])\n",
        "t = 0.001\n",
        "tau = 1e-1\n",
        "max_iter = 10000000\n",
        "\n",
        "# Gradient descent\n",
        "x = x0\n",
        "k = 0\n",
        "while np.linalg.norm(grad_q1(x)) > tau and k < max_iter:\n",
        "    x = x - alpha(t) * grad_q1(x)\n",
        "    k += 1\n",
        "\n",
        "# Output the result\n",
        "print(\"Optimal solution x*:\", x)\n",
        "print(\"Number of iterations:\", k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyJomiKKEl-r",
        "outputId": "84eb9208-5bcb-4a54-c04f-3c67dae2c4be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution x*: [450425.05585283 -14229.45987838]\n",
            "Number of iterations: 3460023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the results obtained by both methods:\n",
        "\n",
        "***Nesterov Accelerated Gradient Descent (NAGD):***\n",
        "\n",
        "***Optimal solution*** $\\mathbf{x^*}$: $[500499.95734358, -15811.38695327]$\n",
        "\n",
        "***Number of iterations***: $175243$\n",
        "\n",
        "***Gradient Descent:***\n",
        "\n",
        "***Optimal solution*** $\\mathbf{x^*}$: $[450425.05585283, -14229.45987838]$\n",
        "\n",
        "***Number of iterations***: $3460023$\n",
        "\n",
        "***Observations:***\n",
        "\n",
        "***Optimal Solution:*** The optimal solution obtained by Nesterov Accelerated Gradient Descent (NAGD) is $[500499.95734358, -15811.38695327]$, while the one obtained by the gradient descent method is $[450425.05585283, -14229.45987838]$. The solution obtained by NAGD has higher values for both components of $\\mathbf{x}$, indicating that it reaches a different local minimum compared to gradient descent.\n",
        "\n",
        "***Number of Iterations:*** NAGD required $175243$ iterations to converge to its solution, while gradient descent required $3460023$ iterations. NAGD converged in significantly fewer iterations compared to gradient descent. This demonstrates the efficiency of Nesterov Accelerated Gradient Descent in converging to a solution.\n",
        "\n",
        "***Convergence Behavior:*** The difference in the number of iterations and the obtained solutions suggests that NAGD might have found a more favorable optimization path or exploited the momentum parameter to escape local minima more effectively compared to gradient descent.\n",
        "\n",
        "Overall, Nesterov Accelerated Gradient Descent seems to have provided a more efficient and effective optimization process, leading to a solution with higher values and in significantly fewer iterations compared to the basic gradient descent method.\n",
        "\n"
      ],
      "metadata": {
        "id": "cAGQL9udJ0a6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***PArt(c)***"
      ],
      "metadata": {
        "id": "n2-CqBrsn_P7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function q2(x)\n",
        "def q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return 0.5 * x.T.dot(A.dot(x)) - b2.T.dot(x)\n",
        "\n",
        "# Define the gradient of q2(x)\n",
        "def grad_q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return A.dot(x) - b2\n",
        "\n",
        "# Nesterov's Accelerated Gradient Descent (NAGD) algorithm\n",
        "def nesterov_accelerated_gradient_descent(x0, alpha, beta, tolerance):\n",
        "    x = x0\n",
        "    x_prev = x0\n",
        "    t = 0\n",
        "    while np.linalg.norm(grad_q2(x)) > tolerance:\n",
        "        grad_pert = grad_q2(x + beta * (x - x_prev))\n",
        "        x_next = x - alpha * grad_pert + beta * (x - x_prev)\n",
        "        x_prev = x\n",
        "        x = x_next\n",
        "        t += 1\n",
        "    return x, t\n",
        "\n",
        "# Parameters\n",
        "x0 = np.array([3, 5])\n",
        "alpha = 2 / (7 + np.sqrt(5))\n",
        "beta = np.sqrt((7 + np.sqrt(5)) / (7 - np.sqrt(5))) - 1 / np.sqrt((7 + np.sqrt(5)) / (7 - np.sqrt(5)))\n",
        "tolerance = 1e-4\n",
        "\n",
        "# Run NAGD algorithm\n",
        "optimal_solution1, num_iterations1 = nesterov_accelerated_gradient_descent(x0, alpha, beta, tolerance)\n",
        "\n",
        "# Print results\n",
        "print(\"Optimal solution x*: \", optimal_solution1)\n",
        "print(\"Number of iterations: \", num_iterations1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlB_-awLHBOo",
        "outputId": "1fc55c80-c54e-466a-ef27-70c18e46aacd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution x*:  [0.09089681 0.63638351]\n",
            "Number of iterations:  18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function for neta = 0.7\n",
        "import numpy as np\n",
        "\n",
        "# Define the function q2(x)\n",
        "def q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return 0.5 * x.T.dot(A.dot(x)) - b2.T.dot(x)\n",
        "\n",
        "# Define the gradient of q2(x)\n",
        "def grad_q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return A.dot(x) - b2\n",
        "\n",
        "# Nesterov's Accelerated Gradient Descent (NAGD) algorithm\n",
        "def nesterov_accelerated_gradient_descent1(x0, alpha, tolerance):\n",
        "    x = x0\n",
        "    x_prev = x0\n",
        "    t = 0\n",
        "    while np.linalg.norm(grad_q2(x)) > tolerance:\n",
        "        grad_pert = grad_q2(x + 0.7 * (x - x_prev))\n",
        "        x_next = x - alpha * grad_pert + 0.7 * (x - x_prev)\n",
        "        x_prev = x\n",
        "        x = x_next\n",
        "        t += 1\n",
        "    return x, t\n",
        "\n",
        "# Parameters\n",
        "x0 = np.array([3, 5])\n",
        "alpha = 2 / (7 + np.sqrt(5))\n",
        "\n",
        "tolerance = 1e-4\n",
        "\n",
        "# Run NAGD algorithm\n",
        "optimal_solution2, num_iterations2 = nesterov_accelerated_gradient_descent1(x0, alpha, tolerance)\n",
        "\n",
        "# Print results\n",
        "print(\"Optimal solution x*: \", optimal_solution2)\n",
        "print(\"Number of iterations: \", num_iterations2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQRKDzsSxcxs",
        "outputId": "68caa06b-738a-4ae9-eb91-0f6da4b77505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution x*:  [0.09089743 0.63638251]\n",
            "Number of iterations:  18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here We can observe That for n=0.7 The results are same"
      ],
      "metadata": {
        "id": "VLU5ltx6zBzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the function q2(x)\n",
        "def q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return 0.5 * x.T.dot(A.dot(x)) - b2.T.dot(x)\n",
        "\n",
        "# Define the gradient of q2(x)\n",
        "def grad_q2(x):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return A.dot(x) - b2\n",
        "\n",
        "# Gradient Descent algorithm\n",
        "def gradient_descent(x0, alpha, tolerance):\n",
        "    x = x0\n",
        "    t = 0\n",
        "    while np.linalg.norm(grad_q2(x)) > tolerance:\n",
        "        x -= alpha * grad_q2(x)\n",
        "        t += 1\n",
        "    return x, t\n",
        "\n",
        "# Parameters\n",
        "x0 = np.array([3.0, 5.0])\n",
        "alpha1 = 2 / (7 + np.sqrt(5))\n",
        "tolerance = 1e-2\n",
        "\n",
        "# Run Gradient Descent algorithm\n",
        "optimal_solution2, num_iterations2 = gradient_descent(x0, alpha1, tolerance)\n",
        "\n",
        "# Print results\n",
        "print(\"Optimal solution x*: \", optimal_solution2)\n",
        "print(\"Number of iterations: \", num_iterations2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq38xuRWOZCf",
        "outputId": "8f399673-c872-40f9-82e8-949dd8a7b703"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal solution x*:  [0.08923038 0.63907984]\n",
            "Number of iterations:  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimal Solution:\n",
        "\n",
        "For Nesterov's accelerated gradient descent, the optimal solution is approximately\n",
        "[0.09089681, 0.63638351].\n",
        "For gradient descent, the optimal solution is approximately\n",
        "[0.08923038, 0.63907984].\n",
        "The solutions obtained by both algorithms are very close to each other, indicating that both methods effectively found a solution close to the minimum of the function q2(x).\n",
        "\n",
        "Number of Iterations:\n",
        "\n",
        "Nesterov's accelerated gradient descent required 18 iterations.\n",
        "Gradient descent required only 9 iterations.\n",
        "Gradient descent converged in fewer iterations compared to Nesterov's accelerated gradient descent. This suggests that gradient descent took larger steps towards the minimum of the function and hence converged faster.\n",
        "\n",
        "Observations:\n",
        "\n",
        "- Nesterov's accelerated gradient descent typically takes smaller steps compared to regular gradient descent due to the introduction of the momentum parameter. This can lead to slower convergence but may offer more stability, especially in scenarios with noisy or ill-conditioned objective functions.\n",
        "- Gradient descent without momentum tends to take larger steps and might overshoot the minimum, but it converges faster in this particular scenario where the function is smooth and well-behaved.\n",
        "\n",
        "Overall Behavior:\n",
        "\n",
        "- Both algorithms effectively minimized the function q2(x) to a similar extent, yielding solutions that are very close.\n",
        "- Nesterov's accelerated gradient descent showed a more cautious approach with smaller steps, resulting in a slightly slower convergence compared to gradient descent.\n",
        "- Gradient descent, on the other hand, converged faster but might be more susceptible to oscillations or overshooting in scenarios with highly non-linear or noisy objective functions.\n"
      ],
      "metadata": {
        "id": "AcsDnMiqQu5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rosenbrock_function(x_vector):\n",
        "    return 100*(x_vector[1]-x_vector[0]**2)**2 + (0.5-x_vector[0])**2\n",
        "\n",
        "def gradient_rosenbrock(x_vector):\n",
        "    return np.array([-400*x_vector[0]*(x_vector[1]-x_vector[0]**2)-2*(0.5-x_vector[0]), 200*(x_vector[1]-x_vector[0]**2) ])\n",
        "\n",
        "def project_to_unit_ball(x_initial):\n",
        "    if np.linalg.norm(x_initial) <= 1:\n",
        "        return x_initial\n",
        "    else:\n",
        "        return x_initial / np.linalg.norm(x_initial)\n",
        "\n",
        "def adaptive_gradient_descent(x_initial_guess, max_iterations, learning_rate):\n",
        "    x_current = np.copy(x_initial_guess)\n",
        "    x_sequence = []\n",
        "    x_sequence.append(x_initial_guess)\n",
        "\n",
        "    squared_gradient_norm_sum = np.linalg.norm(gradient_rosenbrock(x_current))**2\n",
        "\n",
        "    for iteration in range(max_iterations):\n",
        "        step_size = learning_rate / np.sqrt(squared_gradient_norm_sum)\n",
        "        next_x = x_current - step_size * gradient_rosenbrock(x_current)\n",
        "\n",
        "        x_current = project_to_unit_ball(next_x)\n",
        "        squared_gradient_norm_sum += np.linalg.norm(gradient_rosenbrock(x_current))**2\n",
        "\n",
        "        x_sequence.append(x_current)\n",
        "\n",
        "    return x_current, rosenbrock_function(x_current), x_sequence\n",
        "\n",
        "# Example usage:\n",
        "initial_guess = np.array([1.0, 1.0])\n",
        "max_iter = 1000\n",
        "learning_rate = 0.1\n",
        "result, minimum_value, sequence = adaptive_gradient_descent(initial_guess, max_iter, learning_rate)\n",
        "print(\"Minimum point:\", result)\n",
        "print(\"Minimum value:\", minimum_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYsHNU_GRurG",
        "outputId": "e7463c65-955e-46a6-9e7f-13c3dd16a5fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimum point: [0.65954494 0.43576855]\n",
            "Minimum value: 0.025513727642272492\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = np.array([0,0])\n",
        "max_iter = [10**2, 500, 10**3, 5000, 10**4, 50000, 10**5, 500000, 10**6, 5000000]\n",
        "\n",
        "\n",
        "for t in max_iter:\n",
        "  R = 2\n",
        "  minimizer, minimum, xks = adaptive_gradient_descent(x0, t, R)\n",
        "  print('------------------------------------------------------')\n",
        "  print('Max no of iterations = ',t)\n",
        "  print('Minimizer = ',minimizer)\n",
        "  print('Final value',minimum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vq2DisJ7nEaN",
        "outputId": "f5dcb626-6cdc-447a-c52c-4de43fba13c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------\n",
            "Max no of iterations =  100\n",
            "Minimizer =  [-0.1191874   0.01564742]\n",
            "Final value 0.383600910872348\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  500\n",
            "Minimizer =  [0.43711819 0.19075968]\n",
            "Final value 0.003963895592601297\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  1000\n",
            "Minimizer =  [0.49185363 0.24187917]\n",
            "Final value 6.652998985998079e-05\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  5000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 2.207963373434313e-18\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  10000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  50000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  100000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  500000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  1000000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 1.262177448353619e-29\n",
            "------------------------------------------------------\n",
            "Max no of iterations =  5000000\n",
            "Minimizer =  [0.5  0.25]\n",
            "Final value 1.262177448353619e-29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def f1(x_vector):\n",
        "    t = 5*x_vector[0] - 5\n",
        "    m = 5*x_vector[1] - 5\n",
        "    return np.sin(m)*np.exp((1-np.cos(t))**2) + np.cos(t)*np.exp((1-np.sin(m))**2) + (t-m)**2\n",
        "\n",
        "def grad_f1(x_vector):\n",
        "    t = 5*x_vector[0] - 5\n",
        "    m = 5*x_vector[1] - 5\n",
        "    return np.array([np.sin(m)*np.exp((1-np.cos(t))**2)*10*np.sin(t)*(1-np.cos(t)) - 5*np.exp((1-np.sin(m))**2)*np.sin(t) + 10*(t-m),\n",
        "                     np.cos(m)*np.exp((1-np.cos(t))**2)*5 - 10*np.cos(t)*np.cos(m)*(1-np.sin(m))*np.exp((1-np.sin(m))**2) - 10*(t-m)])\n",
        "\n",
        "def projection(x_initial):\n",
        "    if np.linalg.norm(x_initial) <= 1:\n",
        "        return x_initial\n",
        "    else:\n",
        "        return x_initial / np.linalg.norm(x_initial)\n",
        "\n",
        "def adaptive_gradient_descent1(x_initial_guess, max_iterations, R_value):\n",
        "    x_current = np.copy(x_initial_guess)\n",
        "    x_sequence = []\n",
        "    x_sequence.append(x_initial_guess)\n",
        "    squared_grad_norm_sum = np.linalg.norm(grad_f1(x_current))**2\n",
        "\n",
        "    for _ in range(max_iterations):\n",
        "        alpha_value = R_value / np.sqrt(squared_grad_norm_sum)\n",
        "        next_y = x_current - alpha_value * grad_f1(x_current)\n",
        "        x_current = projection(next_y)\n",
        "        squared_grad_norm_sum += np.linalg.norm(grad_f1(x_current))**2\n",
        "        x_sequence.append(x_current)\n",
        "\n",
        "    return x_current, f1(x_current), x_sequence\n",
        "\n",
        "x_initial = np.array([0, 0])\n",
        "max_iterations_list = [10**2, 500, 10**3, 5000, 10**4, 50000, 10**5, 500000, 10**6]\n",
        "R_value = 2\n",
        "\n",
        "for max_iter in max_iterations_list:\n",
        "    minimizer, minimum_value, x_sequence = adaptive_gradient_descent1(x_initial, max_iter, R_value)\n",
        "    print('------------------------------------------------------')\n",
        "    print('Max number of iterations =', max_iter)\n",
        "    print('Minimizer =', minimizer)\n",
        "    print('Final value =', minimum_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ59XYwOn0JF",
        "outputId": "f08da506-8c64-4161-ce16-1bdf895a6f3d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------\n",
            "Max number of iterations = 100\n",
            "Minimizer = [-0.89293896 -0.45017776]\n",
            "Final value = -67.7120096660121\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 500\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414144\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 1000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 5000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 10000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.8942207841415\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 50000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414145\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 100000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.8942207841415\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 500000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.8942207841415\n",
            "------------------------------------------------------\n",
            "Max number of iterations = 1000000\n",
            "Minimizer = [-0.8381429  -0.54545071]\n",
            "Final value = -97.89422078414155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def q2(x_vector):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return 0.5 * x_vector @ A @ x_vector - b2 @ x_vector\n",
        "\n",
        "def grad_q2(x_vector):\n",
        "    A = np.array([[4, 1], [1, 3]])\n",
        "    b2 = np.array([1, 2])\n",
        "    return A @ x_vector - b2\n",
        "\n",
        "def hess_q2():\n",
        "    return np.array([[4, 1], [1, 3]])\n",
        "def grad_descent(x0, alpha, tau):\n",
        "    x = np.copy(x0)\n",
        "    k = 0\n",
        "    functional_values = []\n",
        "\n",
        "    while np.linalg.norm(grad_q2(x)) > tau:\n",
        "        x = x- alpha * grad_q2(x)\n",
        "        k = k+1\n",
        "        functional_values.append(q2(x))\n",
        "\n",
        "    return k,x,q2(x),functional_values\n",
        "\n",
        "def conjugate_gradient(x_initial_guess, tolerance):\n",
        "    r = -1 * grad_q2(x_initial_guess)\n",
        "    d = np.copy(r)\n",
        "    x_current = np.copy(x_initial_guess)\n",
        "    k = 0\n",
        "    x_sequence = []\n",
        "\n",
        "    while np.linalg.norm(r) > tolerance:\n",
        "        alpha = r @ r / (d @ hess_q2() @ d)\n",
        "        x_current += alpha * d\n",
        "        r_next = r - alpha * (hess_q2() @ d)\n",
        "        beta = -(r_next @ r_next) / (r @ r)\n",
        "        d = r_next - beta * d\n",
        "        r = np.copy(r_next)\n",
        "        k += 1\n",
        "        x_sequence.append(x_current)\n",
        "\n",
        "    return x_current, q2(x_current), k\n",
        "\n",
        "tolerance = 1e-8\n",
        "initial_point = np.array([5., 3.])\n",
        "minimizer_cg, final_value_cg, iterations_cg = conjugate_gradient(initial_point, tolerance)\n",
        "\n",
        "# For comparison\n",
        "alpha = 2 / (7 + np.sqrt(5))\n",
        "iteration_gd, minimizer_gd, final_value_gd, function_value_gd = grad_descent(initial_point, alpha, tolerance)\n",
        "\n",
        "print('Type = Conjugate Gradient')\n",
        "print('Minimizer =', minimizer_cg)\n",
        "print('Final value =', final_value_cg)\n",
        "print('Number of iterations =', iterations_cg)\n",
        "print('----------------------------------------------------------------------------------')\n",
        "print('Type = Normal Gradient Descent')\n",
        "print('Iteration =', iteration_gd)\n",
        "print('Minimizer =', minimizer_gd)\n",
        "print('Final value =', final_value_gd)\n",
        "print('----------------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kHAtdnbp4PE",
        "outputId": "90638087-0109-4b4c-948b-fc1cc821f21e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type = Conjugate Gradient\n",
            "Minimizer = [0.09090909 0.63636364]\n",
            "Final value = -0.6818181818181819\n",
            "Number of iterations = 2\n",
            "----------------------------------------------------------------------------------\n",
            "Type = Normal Gradient Descent\n",
            "Iteration = 26\n",
            "Minimizer = [0.09090909 0.63636363]\n",
            "Final value = -0.6818181818181818\n",
            "----------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}