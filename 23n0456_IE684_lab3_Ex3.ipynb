{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Answer 3:\n",
        "part(a)\n",
        "\n",
        "For $f(x)$:\n",
        "$\n",
        "\\frac{\\partial f}{\\partial x_i} = 8(x_i - 1) + 8(x_{2i}^2 - x_{i+1}) = 0\n",
        "$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$\n",
        "\\frac{\\partial f}{\\partial x_i} \\Big|_{x_i=1} = 8(1 - 1) + 8(x_{2i}^2 - x_{i+1}) = 0\n",
        "$\n",
        "\n",
        "This simplifies to $-8(x_{2i}^2 - x_{i+1}) = 0$. Since $x_{2i}^2$ is always non-negative, the only solution is $x_{i+1} = 1$.\n",
        "\n",
        "For $g(x)$:\n",
        "$\n",
        "\\frac{\\partial g}{\\partial x_i} = 2(x_1 - x_{2i}) - 2(x_{2i-1} - x_i) + 2(x_i - 1) = 0\n",
        "$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$\n",
        "\\frac{\\partial g}{\\partial x_i} \\Big|_{x_i=1} = 2(x_1 - x_{2i}) - 2(x_{2i-1} - 1) + 2(1 - 1) = 0\n",
        "$\n",
        "\n",
        "This simplifies to $2(x_1 - x_{2i}) - 2(x_{2i-1} - 1) = 0$. Similarly, since $x_{2i-1} \\geq 1$ and $x_{2i} \\geq 1$, the only solution is $x_1 = 1$.\n",
        "\n",
        "Therefore, $x_i = 1$ for all $i$ is indeed a solution to both systems of equations, confirming that $x_i = 1$ is a minimizer for both $f(x)$ and $g(x)$.\n",
        "\n",
        "For $f(x)$:\n",
        "$f(x) = \\sum_{i=1}^{n-1} [4(x_{2i} - x_{i+1})^2 + (x_i - 1)^2]$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$f(x) = \\sum_{i=1}^{n-1} [4(1 - x_{i+1})^2 + (1 - 1)^2]$\n",
        "\n",
        "Simplifying further:\n",
        "$f(x) = 4\\sum_{i=1}^{n-1} (1 - x_{i+1})^2$\n",
        "\n",
        "Since $x_{i+1} = 1$ for all $i$, each term in the sum becomes zero, and the minimum value of $f(x)$ is $0$.\n",
        "\n",
        "For $g(x)$:\n",
        "$g(x) = \\sum_{i=1}^{n} [(x_1 - x_{2i})^2 + (x_i - 1)^2]$\n",
        "\n",
        "Substitute $x_i = 1$:\n",
        "$g(x) = \\sum_{i=1}^{n} [(1 - x_{2i})^2 + (1 - 1)^2]$\n",
        "\n",
        "Simplifying further:\n",
        "$g(x) = \\sum_{i=1}^{n} (1 - x_{2i})^2$\n",
        "\n",
        "Since $x_{2i} = 1$ for all $i$, each term in the sum becomes zero, and the minimum value of $g(x)$ is $0$.\n",
        "\n",
        "Therefore, when $x_i = 1$ for all $i$, both $f(x)$ and $g(x)$ achieve their minimum values of $0$.\n",
        "\n",
        "**Initial Choice of $B_0$:**\n",
        " a common and often effective choice for the initial Hessian approximation ($B_0$) is the identity matrix ($I$).\n",
        "\n",
        "**Justification:**\n",
        "1. **Positivity Definite:** The identity matrix is always positive definite.\n",
        "2. **Simplicity:** Choosing $B_0 = I$ is computationally simple.\n",
        "3. **General Applicability:** It is a general-purpose choice that often performs well across different problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YKnJwMspbFoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PArt 3"
      ],
      "metadata": {
        "id": "t2ZxqEVt0Sz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def g(x):\n",
        "    n = len(x)\n",
        "    result = 0\n",
        "    for i in range(0, n):\n",
        "        result += (x[0] - x[i]**2)**2 + (x[i] - 1)**2\n",
        "    return result\n",
        "\n",
        "def gradient_g(x):\n",
        "    n = len(x)\n",
        "    grad = np.zeros(n)\n",
        "    for i in range(0, n):\n",
        "        grad[0] += 2 * (x[0] - x[i]**2)\n",
        "        grad[i] += -4 * (x[0] - x[i]**2) * x[i] + 2 * (x[i] - 1)\n",
        "    return grad\n",
        "\n"
      ],
      "metadata": {
        "id": "5vKLUbHCkVJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def g(x):\n",
        "    n = len(x)\n",
        "    result = 0\n",
        "    for i in range(0, n):\n",
        "        result += (x[0] - x[i]**2)**2 + (x[i] - 1)**2\n",
        "    return result\n",
        "\n",
        "def gradient_g(x):\n",
        "    n = len(x)\n",
        "    grad = np.zeros(n)\n",
        "    for i in range(0, n):\n",
        "        grad[0] += 2 * (x[0] - x[i]**2)\n",
        "        grad[i] += -4 * (x[0] - x[i]**2) * x[i] + 2 * (x[i] - 1)\n",
        "    return grad\n",
        "\n",
        "def backtracking_line_search_g(x, direction, alpha0, rho, gamma):\n",
        "    alpha = alpha0\n",
        "    while g(x + alpha * direction) > g(x) + gamma * alpha * np.dot(gradient_g(x), direction):\n",
        "        alpha *= rho\n",
        "    return alpha\n",
        "\n",
        "def bfgs_algorithm_g(x0, tolerance, alpha0, rho, gamma):\n",
        "    n = len(x0)\n",
        "    k = 0\n",
        "    Bk = np.eye(n)  # Initial Hessian approximation\n",
        "    x = x0.copy()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    while np.linalg.norm(gradient_g(x)) > tolerance:\n",
        "        pk = -np.dot(Bk, gradient_g(x))\n",
        "        alpha_k = backtracking_line_search_g(x, pk, alpha0, rho, gamma)\n",
        "        x_next = x + alpha_k * pk\n",
        "\n",
        "        sk = x_next - x\n",
        "        yk = gradient_g(x_next) - gradient_g(x)\n",
        "\n",
        "        # BFGS update formula for Hessian approximation\n",
        "        rho_k = 1 / np.dot(yk, sk)\n",
        "        Bk = (np.eye(n) - rho_k * np.outer(sk, yk)) @ Bk @ (np.eye(n) - rho_k * np.outer(yk, sk)) + rho_k * np.outer(sk, sk)\n",
        "\n",
        "        x = x_next\n",
        "        k += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    return x, execution_time\n",
        "\n",
        "# Set parameters for backtracking line search\n",
        "alpha0 = 0.9\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "\n",
        "# Set n values\n",
        "n_values = [1000, 2500, 5000, 7500, 10000]\n",
        "\n",
        "for n in n_values:\n",
        "    x0 = np.zeros(n)\n",
        "    tolerance = 1e-6\n",
        "    result, exec_time = bfgs_algorithm_g(x0, tolerance, alpha0, rho, gamma)\n",
        "    print(f\"Minimizer for n={n}: {result}\")\n",
        "    print(f\"Objective function value: {g(result)}\")\n",
        "    print(f\"Execution time: {exec_time} seconds\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WRmqtNXwtsgS",
        "outputId": "3642e883-8a4d-467c-bf57-2b84804e1d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Minimizer for n=1000: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Objective function value: 1.338611199363682e-17\n",
            "Execution time: 2.816406011581421 seconds\n",
            "\n",
            "Minimizer for n=2500: [1. 1. 1. ... 1. 1. 1.]\n",
            "Objective function value: 2.7701236628258894e-16\n",
            "Execution time: 30.415228366851807 seconds\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a25d69fc799>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtolerance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbfgs_algorithm_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Minimizer for n={n}: {result}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Objective function value: {g(result)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-8a25d69fc799>\u001b[0m in \u001b[0;36mbfgs_algorithm_g\u001b[0;34m(x0, tolerance, alpha0, rho, gamma)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# BFGS update formula for Hessian approximation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mrho_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mBk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho_k\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mBk\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mrho_k\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrho_k\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/twodim_base.py\u001b[0m in \u001b[0;36meye\u001b[0;34m(N, M, k, dtype, order, like)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_array_function_like_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#part 2"
      ],
      "metadata": {
        "id": "SEPfdZDc0YYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def f(x):\n",
        "    n = len(x)\n",
        "    result = 0\n",
        "    for i in range(0, n-1):\n",
        "        result += 4 * (x[i] ** 2 - x[i+1])**2 + (x[i] - 1)**2\n",
        "    return result\n",
        "\n",
        "def gradient_f(x):\n",
        "    n = len(x)\n",
        "    grad = np.zeros(n)\n",
        "    for i in range(0, n-1):\n",
        "        grad[i] += 8 * (x[i] ** 2 - x[i+1]) * 2 * x[i] + 2 * (x[i] - 1)\n",
        "        grad[i+1] += -8 * (x[i] ** 2 - x[i+1]) * 2 * x[i]\n",
        "    return grad\n",
        "\n",
        "def backtracking_line_search_f(x, direction, alpha0, rho, gamma):\n",
        "    alpha = alpha0\n",
        "    while f(x + alpha * direction) > f(x) + gamma * alpha * np.dot(gradient_f(x), direction):\n",
        "        alpha *= rho\n",
        "    return alpha\n",
        "\n",
        "def bfgs_algorithm_f(x0, tolerance, alpha0, rho, gamma):\n",
        "    n = len(x0)\n",
        "    k = 0\n",
        "    Bk = np.eye(n)  # Initial Hessian approximation\n",
        "    x = x0.copy()\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    while np.linalg.norm(gradient_f(x)) > tolerance:\n",
        "        pk = -np.dot(Bk, gradient_f(x))\n",
        "        alpha_k = backtracking_line_search_f(x, pk, alpha0, rho, gamma)\n",
        "        x_next = x + alpha_k * pk\n",
        "\n",
        "        sk = x_next - x\n",
        "        yk = gradient_f(x_next) - gradient_f(x)\n",
        "\n",
        "        # BFGS update formula for Hessian approximation\n",
        "        rho_k = 1 / np.dot(yk, sk)\n",
        "        Bk = (np.eye(n) - rho_k * np.outer(sk, yk)) @ Bk @ (np.eye(n) - rho_k * np.outer(yk, sk)) + rho_k * np.outer(sk, sk)\n",
        "\n",
        "        x = x_next\n",
        "        k += 1\n",
        "\n",
        "    end_time = time.time()\n",
        "    execution_time = end_time - start_time\n",
        "    return x, execution_time\n",
        "\n",
        "# Set parameters for backtracking line search\n",
        "alpha0 = 0.9\n",
        "rho = 0.5\n",
        "gamma = 0.5\n",
        "\n",
        "# Set n values\n",
        "n_values = [1000, 2500, 5000, 7500, 10000]\n",
        "\n",
        "# Initialize a DataFrame to store results\n",
        "results_df = pd.DataFrame(columns=['n', 'Minimizer', 'Objective Function Value', 'Execution Time'])\n",
        "\n",
        "for n in n_values:\n",
        "    x0 = np.zeros(n)\n",
        "    tolerance = 1e-6\n",
        "    result, exec_time = bfgs_algorithm_f(x0, tolerance, alpha0, rho, gamma)\n",
        "    obj_func_value = f(result)\n",
        "\n",
        "    # Append results to DataFrame\n",
        "    results_df = results_df.append({\n",
        "        'n': n,\n",
        "        'Minimizer': result,\n",
        "        'Objective Function Value': obj_func_value,\n",
        "        'Execution Time': exec_time\n",
        "    }, ignore_index=True)\n",
        "\n",
        "# Display the results\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "id": "1gtz1iZ3u0FN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}