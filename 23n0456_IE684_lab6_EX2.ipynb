{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EkmcSdFVv8N",
        "outputId": "a7685867-d204-48f2-a64b-9f62ac848669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For d: 1000\n",
            "Number of iterations: 4\n",
            "Minimum value: 0.14183476870809816 using Newton method with backtracking\n",
            "|| Ax_opt - y ||^2 = 191.7145432980871\n",
            "|| x_opt - xorig ||^2 = 826.8244356809709\n",
            "Time taken for Newton method: 0.8585500717163086\n",
            "For d: 5000\n",
            "Number of iterations: 4\n",
            "Minimum value: 0.10576241777383968 using Newton method with backtracking\n",
            "|| Ax_opt - y ||^2 = 191.7145432980871\n",
            "|| x_opt - xorig ||^2 = 4806.013431133197\n",
            "Time taken for Newton method: 55.547008752822876\n",
            "For d: 10000\n",
            "Number of iterations: 4\n",
            "Minimum value: 0.11443640710309748 using Newton method with backtracking\n",
            "|| Ax_opt - y ||^2 = 191.71454329808705\n",
            "|| x_opt - xorig ||^2 = 9780.120415921312\n",
            "Time taken for Newton method: 392.1974811553955\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function Definitions\n",
        "def fx(x, lambda_reg):\n",
        "    return 0.5 * np.linalg.norm(A @ x - y) ** 2 + 0.5 * lambda_reg * np.dot(x, x)\n",
        "\n",
        "def grad_fx(x, lambda_reg):\n",
        "    return np.dot(A.transpose(), A) @ x - A.transpose() @ y + lambda_reg * x\n",
        "\n",
        "def hessian_fx(lambda_reg):\n",
        "    return A.transpose() @ A + lambda_reg * np.identity(A.shape[1])\n",
        "\n",
        "def dk_fx(lambda_reg):\n",
        "    return np.linalg.inv(hessian_fx(lambda_reg))\n",
        "\n",
        "def get_alpha_newton(xk, alpha0, rho, gamma, Dk, lambda_reg):\n",
        "    alpha = alpha0\n",
        "    pk = -grad_fx(xk, lambda_reg)\n",
        "    while fx(xk + alpha * Dk @ pk, lambda_reg) > (fx(xk, lambda_reg) + gamma * alpha * grad_fx(xk, lambda_reg) @ Dk @ pk):\n",
        "        alpha = rho * alpha\n",
        "    return alpha\n",
        "\n",
        "def newton_method_with_backtracking(x0, tau, alpha0, rho, gamma, lambda_reg):\n",
        "    xk = np.copy(x0)\n",
        "    count = 0\n",
        "    pk = grad_fx(xk, lambda_reg)\n",
        "    xks = [xk]\n",
        "    while np.linalg.norm(pk) > tau:\n",
        "        Dk = dk_fx(lambda_reg)\n",
        "        alpha = get_alpha_newton(xk, alpha0, rho, gamma, Dk, lambda_reg)\n",
        "        xk = xk - alpha * Dk @ pk\n",
        "        pk = grad_fx(xk, lambda_reg)\n",
        "        xks.append(xk)\n",
        "        count += 1\n",
        "    return count, xk, fx(xk, lambda_reg), xks\n",
        "\n",
        "# Experiment Parameters\n",
        "np.random.seed(10)  # for repeatability\n",
        "N = 200\n",
        "ds = [1000, 5000, 10000, 20000, 25000, 50000, 100000, 200000, 500000, 1000000]\n",
        "lambda_reg = 0.001\n",
        "eps = np.random.randn(N, 1)  # random noise\n",
        "\n",
        "# Experiment Execution\n",
        "for i in range(np.size(ds)):\n",
        "    d = ds[i]\n",
        "    A = np.random.randn(N, d)\n",
        "    for j in range(A.shape[1]):\n",
        "        A[:, j] = A[:, j] / np.linalg.norm(A[:, j])\n",
        "\n",
        "    xorig = np.ones((d, 1)).flatten()\n",
        "    x0 = np.zeros(d)\n",
        "    y = np.dot(A, xorig) + eps.flatten()\n",
        "\n",
        "    tau = 1e-5\n",
        "    alpha0 = 0.99\n",
        "    rho = 0.5\n",
        "    gamma = 0.5\n",
        "\n",
        "    start = time.time()\n",
        "    count, minimizer, minimum, _ = newton_method_with_backtracking(x0, tau, alpha0, rho, gamma, lambda_reg)\n",
        "    newtontime = time.time() - start\n",
        "\n",
        "    print(\"For d:\", d)\n",
        "    print(\"Number of iterations:\", count)\n",
        "    print(\"Minimum value:\", minimum, \"using Newton method with backtracking\")\n",
        "    print(\"|| Ax_opt - y ||^2 =\", np.linalg.norm(A @ xorig - y) ** 2)\n",
        "    print(\"|| x_opt - xorig ||^2 =\", np.linalg.norm(minimizer - xorig) ** 2)\n",
        "    print(\"Time taken for Newton method:\", newtontime)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Function Definitions\n",
        "def objective_func(x, lambda_val):\n",
        "    return 0.5 * np.linalg.norm(matrix_A @ x - vector_y) ** 2 + 0.5 * lambda_val * np.dot(x, x)\n",
        "\n",
        "def gradient_func(x, lambda_val):\n",
        "    return np.dot(matrix_A.transpose(), matrix_A) @ x - matrix_A.transpose() @ vector_y + lambda_val * x\n",
        "\n",
        "def hessian_mat(lambda_val):\n",
        "    return matrix_A.transpose() @ matrix_A + lambda_val * np.identity(matrix_A.shape[1])\n",
        "\n",
        "def inv_hessian_mat(lambda_val):\n",
        "    return np.linalg.inv(hessian_mat(lambda_val))\n",
        "\n",
        "def step_size_bfgs(xk, alpha_start, rho_val, gamma_val, B_k, lambda_val):\n",
        "    alpha = alpha_start\n",
        "    pk = -gradient_func(xk, lambda_val)\n",
        "    while objective_func(xk + alpha * B_k @ pk, lambda_val) > (objective_func(xk, lambda_val) + gamma_val * alpha * gradient_func(xk, lambda_val) @ B_k @ pk):\n",
        "        alpha = rho_val * alpha\n",
        "    return alpha\n",
        "\n",
        "def bfgs_method(x_start, tol_val, alpha_start, rho_val, gamma_val, lambda_val):\n",
        "    x_k = np.copy(x_start)\n",
        "    n = len(x_start)\n",
        "    B_k = np.eye(n)\n",
        "    iteration_count = 0\n",
        "    pk = gradient_func(x_k, lambda_val)\n",
        "    x_k_list = [x_k]\n",
        "\n",
        "    while np.linalg.norm(pk) > tol_val:\n",
        "        alpha = step_size_bfgs(x_k, alpha_start, rho_val, gamma_val, B_k, lambda_val)\n",
        "        x_next = x_k - alpha * (B_k @ pk)\n",
        "        sk = x_next - x_k\n",
        "        yk = gradient_func(x_next, lambda_val) - gradient_func(x_k, lambda_val)\n",
        "\n",
        "        B_k = np.dot((np.eye(len(x_k)) - np.outer(sk, yk) / np.dot(yk, sk)), np.dot(B_k, (np.eye(len(x_k)) - np.outer(yk, sk) / np.dot(yk, sk)))) + np.outer(sk, sk) / np.dot(yk, sk)\n",
        "\n",
        "        x_k = x_next\n",
        "        pk = gradient_func(x_k, lambda_val)\n",
        "        x_k_list.append(x_k)\n",
        "        iteration_count += 1\n",
        "\n",
        "    return iteration_count, x_k, objective_func(x_k, lambda_val), x_k_list\n",
        "\n",
        "# Experiment Parameters\n",
        "np.random.seed(10)\n",
        "num_samples = 200\n",
        "dimensions_list = [1000, 5000, 10000, 20000, 25000, 50000, 100000, 200000, 500000, 1000000]\n",
        "lambda_val = 0.001\n",
        "noise_eps = np.random.randn(num_samples, 1)\n",
        "\n",
        "# Experiment Execution\n",
        "for dim_val in dimensions_list:\n",
        "    dim_size = dim_val\n",
        "    matrix_A = np.random.randn(num_samples, dim_size)\n",
        "    for j in range(matrix_A.shape[1]):\n",
        "        matrix_A[:, j] = matrix_A[:, j] / np.linalg.norm(matrix_A[:, j])\n",
        "\n",
        "    x_original = np.ones((dim_size, 1)).flatten()\n",
        "    x_start = np.zeros(dim_size)\n",
        "    vector_y = np.dot(matrix_A, x_original) + noise_eps.flatten()\n",
        "\n",
        "    tolerance_val = 1e-5\n",
        "    alpha_start_val = 0.99\n",
        "    rho_val = 0.5\n",
        "    gamma_val = 0.5\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    iter_count, minimizer, min_val, _ = bfgs_method(x_start, tolerance_val, alpha_start_val, rho_val, gamma_val, lambda_val)\n",
        "    exec_time = time.time() - start_time\n",
        "\n",
        "    print(\"For dimension:\", dim_size)\n",
        "    print(\"Number of iterations:\", iter_count)\n",
        "    print(\"Minimum value:\", min_val, \"using BFGS method\")\n",
        "    print(\"||Ax_opt - y||^2 =\", np.linalg.norm(matrix_A @ x_original - vector_y) ** 2)\n",
        "    print(\"||x_opt - x_original||^2 =\", np.linalg.norm(minimizer - x_original) ** 2)\n",
        "    print(\"Time taken for BFGS method:\", exec_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxZw-Pa_W-pQ",
        "outputId": "f012ca2d-ab7c-4f50-a2c7-524160bb4170"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For dimension: 1000\n",
            "Number of iterations: 32\n",
            "Minimum value: 0.14183476871428477 using BFGS method\n",
            "||Ax_opt - y||^2 = 191.7145432980871\n",
            "||x_opt - x_original||^2 = 826.824435809269\n",
            "Time taken for BFGS method: 9.607851505279541\n",
            "For dimension: 5000\n",
            "Number of iterations: 40\n",
            "Minimum value: 0.10576241777506894 using BFGS method\n",
            "||Ax_opt - y||^2 = 191.7145432980871\n",
            "||x_opt - x_original||^2 = 4806.0134310008425\n",
            "Time taken for BFGS method: 872.1767685413361\n"
          ]
        }
      ]
    }
  ]
}