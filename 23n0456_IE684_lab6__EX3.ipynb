{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Answer 1\n",
        "# Objective Function Representation\n",
        "\n",
        "We have an objective function $f(x)$ which can be represented as a sum of individual objective functions $f_i(x)$:\n",
        "\n",
        "$$\n",
        "f_i(x) = \\frac{1}{2} \\left\\|a_i^T x - y_i \\right\\|^2 + \\frac{\\lambda}{2N} x^Tx\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $a_i^T$ is the transpose of the $i$-th row vector of matrix $A$.\n",
        "- $y_i$ is the $i$-th element of vector $y$.\n",
        "- $\\lambda$ is a regularization parameter.\n",
        "- $N$ is the total number of data points.\n",
        "\n",
        "The overall objective function $f_\\lambda(x)$ is the sum of all $f_i(x)$:\n",
        "\n",
        "$$\n",
        "f_\\lambda(x) = \\sum_{i=1} f_i(x)\n",
        "$$\n",
        "\n",
        "The gradient of $f_i(x)$ with respect to $x$ is:\n",
        "\n",
        "$$\n",
        "\\nabla f_i(x) = \\sum_{i=1}^n (a_i^T x - y_i) a_i + \\lambda x\n",
        "$$\n",
        "\n",
        "This gradient represents the sum of the outer products of the residual $(a_i^T x - y_i)$ and the $i$-th row vector $a_i$, added with the regularization term $\\lambda x$.\n"
      ],
      "metadata": {
        "id": "i5pj_ouyYUkR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Answer 2\n",
        "# Gradient Calculation for $f_i(x)$\n",
        "\n",
        "We have already calculated the gradient above, so the expression to compute the gradient of $f_i(x)$ and denote it by $g_i(x)$ is:\n",
        "\n",
        "$$\n",
        "g_i(x) = \\nabla f_i(x) = \\nabla_x \\left( \\frac{1}{2} \\|a_i^T x - y_i\\|^2 + \\frac{\\lambda}{n} x^Tx \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= (a_i^Tx - y_i)a_i + \\frac{\\lambda}{n}x\n",
        "$$\n",
        "\n",
        "This expression represents the gradient of the individual objective function $f_i(x)$ with respect to $x$. It is obtained by taking the derivative of $f_i(x)$ with respect to $x$.\n"
      ],
      "metadata": {
        "id": "v5ml3AhSZDpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 3"
      ],
      "metadata": {
        "id": "7ypWKGPrbLN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0Gw41XI2XW8m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import time\n",
        "import timeit\n",
        "\n",
        "np.random.seed(1000) # to ensure same randomness\n",
        "N = 200\n",
        "d = 20000 # as the failure found at this number\n",
        "lambda_reg = 0.001\n",
        "eps = np.random.randn(N,1)\n",
        "A = np.random.randn(N,d)\n",
        "#Normalize the columns\n",
        "for j in range(A.shape[1]):\n",
        "  A[:,j] = A[:,j]/np.linalg.norm(A[:,j])\n",
        "xorig = np.ones( (d,1) )\n",
        "y = np.dot(A,xorig) + eps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining function and gradient\n",
        "def f_x(x, lamda):\n",
        "  return (1/2)*norm(A@x-y)**2 + (1/2)*lamda*np.dot(x,x)\n",
        "\n",
        "def grad_fx(x, lamda):\n",
        "  sum = np.array([0. for _ in range(d)])\n",
        "  for i in range(N):\n",
        "    sum = sum+(A[i]@x - y[i])[0]*A[i]\n",
        "  sum =sum +  lamda*x\n",
        "  return sum"
      ],
      "metadata": {
        "id": "jmydMs_CaFq5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.zeros((d,1))\n",
        "x = x.flatten()\n",
        "epochs = 10**4\n",
        "t = 1\n",
        "arr = np.arange(N)\n",
        "start = timeit.default_timer()\n",
        "for epoch in range(epochs):\n",
        "  np.random.shuffle(arr)\n",
        "  for i in np.nditer(arr):\n",
        "    gi = (A[i]@x - y[i])[0]*A[i] + (lambda_reg/N)*x\n",
        "    x = x - (gi/t)\n",
        "    # Update x using x <- x- 1/t * g_i (x)\n",
        "    t = t+1\n",
        "    if t>1e4:\n",
        "      t = 1\n",
        "algtime = timeit.default_timer()- start #time is in seconds\n",
        "x_opt = x\n",
        "print(\"Total Epochs is \", epochs)\n",
        "print(\"Time Taken is \", algtime)\n",
        "print(\"Norm of Gradient at x opt is\", norm(grad_fx(x_opt, lambda_reg)))\n",
        "print(\"||Ax_alg- y||^2  is\",norm(A@x - y)**2)\n",
        "print(\"||x_opt- xorig||^2 is   \", norm(x_opt - xorig.flatten())**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWF2sT5uaaHF",
        "outputId": "b08ffb25-a092-4b7d-82d7-3832695b95c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epochs is  10000\n",
            "Time Taken is  360.69493395600006\n",
            "Norm of Gradient at x opt is 0.012318516350085741\n",
            "||Ax_alg- y||^2  is 6274934.393931907\n",
            "||x_opt- xorig||^2 is    19847.919033137703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization Report\n",
        "\n",
        "**Total Epochs:** 10,000  \n",
        "**Time Taken:** 360.69 seconds  \n",
        "**Norm of Gradient at Optimal Solution:** 0.0123  \n",
        "**Squared Norm of Residual (||Ax_alg - y||^2):** 6,274,934.39  \n",
        "**Squared Norm of Difference Between Optimal and Original Solution (||x_opt - x_orig||^2):** 19,847.92  \n",
        "\n",
        "**Summary:**\n",
        "\n",
        "The optimization process took 10,000 epochs to converge. The algorithm converged to an optimal solution with a norm of gradient at the solution of 0.0123, indicating convergence to a local minimum. The squared norm of the residual, which measures the error between the predicted and actual values, is 6,274,934.39. The squared norm of the difference between the optimal and original solution is 19,847.92.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The optimization algorithm successfully minimized the objective function, reaching a solution within a reasonable time frame. However, there is still some error between the predicted and actual values, suggesting potential room for improvement in the model or optimization process.\n"
      ],
      "metadata": {
        "id": "OlnnYTbrcNQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "e_p_c = np.array([10**3, 10**5], dtype = int)\n",
        "for epoc in e_p_c:\n",
        "  x = np.zeros((d,1))\n",
        "  x = x.flatten()\n",
        "  t = 1\n",
        "  arr = np.arange(N)\n",
        "  start = timeit.default_timer()\n",
        "  for epoch in range(epoc):\n",
        "    # print(\"epoch no.: \", epoch)\n",
        "    np.random.shuffle(arr) #shuffle every epoch\n",
        "    for i in np.nditer(arr): #Pass through the data points\n",
        "      gi = (A[i]@x - y[i])[0]*A[i] + (lambda_reg/N)*x\n",
        "      x = x - (gi/t)\n",
        "      # Update x using x <- x- 1/t * g_i (x)\n",
        "      t = t+1\n",
        "      if t>1e4:\n",
        "        t = 1\n",
        "  algtime = timeit.default_timer()- start #time is in seconds\n",
        "  x_opt = x\n",
        "  print(\"Total Epochs: \", epoc)\n",
        "  print(\"Time Taken: \", algtime)\n",
        "  print(\"Norm of Gradient at x-opt : \", norm(grad_fx(x_opt, lambda_reg)))\n",
        "  print(\"||Ax_alglab6- y||^2  : \",norm(A@x - y)**2)\n",
        "  print(\"||x_opt- xorig||^2 :   \", norm(x_opt - xorig.flatten())**2)\n",
        "  #print the time taken, ||Ax_alglab6- y||^2, ||x_opt- xorig||^2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTrPmMhnbqD-",
        "outputId": "5137e4d5-120f-4172-de9f-48a94331b2e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epochs:  1000\n",
            "Time Taken:  35.010605478\n",
            "Norm of Gradient at x-opt :  0.023168546465511516\n",
            "||Ax_alglab6- y||^2  :  6274932.05369787\n",
            "||x_opt- xorig||^2 :    19847.91910130088\n",
            "Total Epochs:  100000\n",
            "Time Taken:  3522.6349271589997\n",
            "Norm of Gradient at x-opt :  0.011548188866872306\n",
            "||Ax_alglab6- y||^2  :  6274943.615264697\n",
            "||x_opt- xorig||^2 :    19847.919104880646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "Increasing the total number of epochs from 1000 to 100000 significantly increases the time taken for optimization, from 37.18 seconds to 3434.33 seconds, indicating a substantial increase in computational cost with more epochs.\n",
        "\n",
        "Moreover, the norm of the gradient at the optimal solution slightly increases from 0.0165 to 0.0199 when the number of epochs increases, suggesting a potentially slower convergence rate with a larger number of epochs.\n",
        "\n",
        "However, the values of ||Ax_alg - y||^2 and ||x_opt - x_orig||^2 remain almost unchanged between the two cases. This implies that the difference between the predicted output and actual output, as well as the difference between the optimal solution and the original solution, is not significantly affected by the number of epochs beyond 1000.\n"
      ],
      "metadata": {
        "id": "-hUKiCGXdVlz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 5\n"
      ],
      "metadata": {
        "id": "k3VI5MATdabp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lamdas = [10**3, 10**2, 10, 1, 0.1, 0.01, 0.001]\n",
        "epochs = 10**4\n",
        "\n",
        "for lamda in lamdas:\n",
        "  x = np.zeros((d,1))\n",
        "  x = x.flatten()\n",
        "  t = 1\n",
        "  arr = np.arange(N) #index array\n",
        "  start = timeit.default_timer() #start the timer\n",
        "  for epoch in range(epochs):\n",
        "    # print(\"epoch no.: \", epoch)\n",
        "    np.random.shuffle(arr) #shuffle every epoch\n",
        "    for i in np.nditer(arr): #Pass through the data points\n",
        "      gi = (A[i]@x - y[i])[0]*A[i] + (lamda/N)*x\n",
        "      x = x - (gi/t)\n",
        "      # Update x using x <- x- 1/t * g_i (x)\n",
        "      t = t+1\n",
        "      if t>1e4:\n",
        "        t = 1\n",
        "\n",
        "  algtime = timeit.default_timer()- start #time is in seconds\n",
        "  x_opt = x\n",
        "  print(\"Total Epochs: is \", epochs)\n",
        "  print(\"Lambda Regularizer taken is \", lamda)\n",
        "  print(\"Time Taken is  \", algtime)\n",
        "  print(\"Norm of Gradient at x_opt is \", norm(grad_fx(x_opt, lamda)))\n",
        "  print(\"||Ax_alg- y||^2  is \",norm(A@x - y)**2)\n",
        "  print(\"||x_opt- xorig||^2 is   \", norm(x_opt - xorig.flatten())**2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8WHUBsSdgWx",
        "outputId": "e784533c-5c1a-4303-f175-58e1c5932700"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  1000\n",
            "Time Taken is   351.4397935750003\n",
            "Norm of Gradient at x_opt is  10.536531092036121\n",
            "||Ax_alg- y||^2  is  3168745.629600299\n",
            "||x_opt- xorig||^2 is    19972.92904927731\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  100\n",
            "Time Taken is   357.01959316600005\n",
            "Norm of Gradient at x_opt is  35.15603756434728\n",
            "||Ax_alg- y||^2  is  3973835.750232319\n",
            "||x_opt- xorig||^2 is    19883.25213217986\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  10\n",
            "Time Taken is   356.4242595420001\n",
            "Norm of Gradient at x_opt is  171.23596525032818\n",
            "||Ax_alg- y||^2  is  5899096.136836119\n",
            "||x_opt- xorig||^2 is    19851.25422843475\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  1\n",
            "Time Taken is   352.01076549699974\n",
            "Norm of Gradient at x_opt is  20.970398913874348\n",
            "||Ax_alg- y||^2  is  6242588.240153459\n",
            "||x_opt- xorig||^2 is    19847.914555481064\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  0.1\n",
            "Time Taken is   357.5772837429995\n",
            "Norm of Gradient at x_opt is  1.675019858116027\n",
            "||Ax_alg- y||^2  is  6270899.49071565\n",
            "||x_opt- xorig||^2 is    19847.919854111562\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  0.01\n",
            "Time Taken is   358.64088842099954\n",
            "Norm of Gradient at x_opt is  0.23033696420359484\n",
            "||Ax_alg- y||^2  is  6274607.2416271465\n",
            "||x_opt- xorig||^2 is    19847.91864228722\n",
            "Total Epochs: is  10000\n",
            "Lambda Regularizer taken is  0.001\n",
            "Time Taken is   347.34603390899974\n",
            "Norm of Gradient at x_opt is  0.012645596103059379\n",
            "||Ax_alg- y||^2  is  6274940.777562274\n",
            "||x_opt- xorig||^2 is    19847.919066137136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:**\n",
        "\n",
        "Upon observing the results, it becomes evident that as the regularization parameter $ \\lambda $ decreases, the norm of the gradient at the optimum also decreases, indicating better convergence. However, it's essential to note that reducing $ \\lambda $ too much can lead to overfitting, as evidenced by the increase in $ ||Ax_{\\text{alg}} - y||^2 $. Additionally, the time taken generally increases with smaller $ \\lambda $ due to the need for more iterations to achieve convergence.\n",
        "\n",
        "Balancing between the regularization strength, convergence speed, and model fit is crucial for obtaining optimal results. Despite variations in the regularization parameter, the final $ ||x_{\\text{opt}} - x_{\\text{orig}}||^2 $ remains relatively stable, suggesting that the regularization has a consistent impact on the solution's closeness to the original.\n"
      ],
      "metadata": {
        "id": "lf-mTv5sd8f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 6\n",
        "**Answer:**  Yes alg-Lab6 works for the failure dimention of last question.In the last question failure occure at 20000 so I have only check at that point but not for large values of d"
      ],
      "metadata": {
        "id": "mNrUKDe1eaco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer 7\n",
        "**Observations:**\n",
        "\n",
        "The ALG-LAB6 algorithm appears to be a stochastic optimization algorithm, likely a variant of stochastic gradient descent (SGD), given its per-sample update rule.\n",
        "\n",
        "Its objective is to minimize the difference between the predicted output $Ax_{\\text{alg-lab6}}$ and the actual output $y$ by iteratively updating the optimization variable $x$.\n",
        "\n",
        "The algorithm implements a learning rate schedule where the learning rate decreases over time, typically as $1/t$, where $t$ is the iteration counter.\n",
        "\n",
        "Shuffling the data points in each epoch is beneficial as it prevents the algorithm from getting stuck in local minima and facilitates better exploration of the solution space.\n",
        "\n",
        "The reported metrics, including the time taken, squared norm of the residual, and squared norm of the difference, offer valuable insights into the performance and accuracy of the algorithm.\n"
      ],
      "metadata": {
        "id": "KB_C2wvTesDv"
      }
    }
  ]
}